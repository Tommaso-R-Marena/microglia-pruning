{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microglia-Inspired Dynamic Pruning - Full Experiment\n",
    "\n",
    "This notebook runs the complete pruning experiment on Phi-3-Mini with GSM8K.\n",
    "\n",
    "**What we're doing:** Training small \"agent\" networks to learn which attention heads can be pruned during inference. Inspired by how microglia prune synapses in the brain.\n",
    "\n",
    "**Expected results:**\n",
    "- 20-30% of attention heads pruned\n",
    "- ~15% latency improvement\n",
    "- <2% accuracy loss\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Tommaso-R-Marena/microglia-pruning/blob/main/notebooks/microglia_pruning_demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This takes ~2 minutes on a T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any existing clone and get fresh copy\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "if os.path.exists('/content/microglia-pruning'):\n",
    "    shutil.rmtree('/content/microglia-pruning')\n",
    "    print('Removed old clone')\n",
    "\n",
    "# Clone repo with latest code\n",
    "!git clone https://github.com/Tommaso-R-Marena/microglia-pruning.git\n",
    "%cd microglia-pruning\n",
    "\n",
    "# Verify we have latest commit\n",
    "!git log --oneline -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers accelerate bitsandbytes peft datasets scipy numpy tqdm matplotlib\n",
    "!pip install -q fvcore\n",
    "\n",
    "print('Installation complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import time\n",
    "\n",
    "sys.path.insert(0, '/content/microglia-pruning')\n",
    "from src.system import MicrogliaPruningSystem\n",
    "\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load Base Model\n",
    "\n",
    "We're using Phi-3-Mini (3.8B parameters). Downloads ~7.5 GB (takes ~3-5 min first time).\n",
    "\n",
    "**You should see:** 'Fixing Phi-3 EOS token issue...' message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = MicrogliaPruningSystem(\n",
    "    model='microsoft/phi-3-mini-4k-instruct',\n",
    "    num_heads=32,\n",
    "    hidden_dim=128,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print('\\nSystem initialized!')\n",
    "print(f'\\nModel size: {sum(p.numel() for p in system.model.parameters())/1e9:.2f}B parameters')\n",
    "print(f'Agent size: {sum(p.numel() for p in system.agents.parameters())/1e6:.2f}M parameters')\n",
    "print(f'\\nAgent overhead: {sum(p.numel() for p in system.agents.parameters())/sum(p.numel() for p in system.model.parameters())*100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Test Baseline Performance\n",
    "\n",
    "Test baseline model on simple math problems.\n",
    "\n",
    "**You should see proper answers** (not garbage output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\n",
    "    'A store sells apples for $2 each. If Sarah buys 5 apples, how much does she spend?',\n",
    "    'John has 15 candies. He gives 3 to each of his 4 friends. How many candies does he have left?',\n",
    "    'A rectangle has a length of 8 meters and a width of 5 meters. What is its area?'\n",
    "]\n",
    "\n",
    "print('Testing baseline (unpruned) model:\\n')\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    prompt = f'Question: {question}\\nAnswer:'\n",
    "    start = time.time()\n",
    "    output = system.generate(prompt, max_new_tokens=100)\n",
    "    elapsed = time.time() - start\n",
    "    answer = output.split('Answer:')[-1].strip()[:200]\n",
    "    print(f'Q{i}: {question}')\n",
    "    print(f'A{i}: {answer}')\n",
    "    print(f'Time: {elapsed:.2f}s\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Train Pruning Agents\n",
    "\n",
    "Train agents to learn which heads to prune.\n",
    "\n",
    "**Training time:** ~15-20 minutes on T4 for 3 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system.train(\n",
    "    dataset_name='gsm8k',\n",
    "    num_epochs=3,\n",
    "    batch_size=2,\n",
    "    learning_rate=1e-4,\n",
    "    alpha_schedule=(0.01, 0.2),\n",
    "    use_lora=False\n",
    ")\n",
    "\n",
    "print('\\nTraining complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = system.training_history\n",
    "\n",
    "if history:\n",
    "    epochs = range(1, len(history) + 1)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    ax1.plot(epochs, [h['task_loss'] for h in history], 'b-o', linewidth=2, markersize=8)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Task Loss', fontsize=12)\n",
    "    ax1.set_title('Math Problem Solving Performance', fontsize=13, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.plot(epochs, [h['sparsity_loss'] for h in history], 'r-o', linewidth=2, markersize=8)\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Sparsity Loss', fontsize=12)\n",
    "    ax2.set_title('Pruning Pressure', fontsize=13, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No training history available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Evaluate Pruned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = system.evaluate(\n",
    "    dataset_name='gsm8k',\n",
    "    split='test',\n",
    "    max_samples=200\n",
    ")\n",
    "\n",
    "print('\\n' + '='*50)\n",
    "print('FINAL RESULTS')\n",
    "print('='*50)\n",
    "print(f\"Accuracy: {results['accuracy']:.1%}\")\n",
    "print(f\"Correct: {results['correct']}/{results['total']}\")\n",
    "print(f\"Sparsity: {results['sparsity']:.1%} heads pruned\")\n",
    "print('='*50)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
